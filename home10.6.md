## Домашнее задание к занятию 17 «Инцидент-менеджмент»
---
### Основная часть
Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

### Ответ
|Шаг|Описание|
|---|--------|
|Краткое описание инцидента|В 22:52 (21.10.18) по UTC на нескольких сервисах GitHub.com пострадали несколько сетевых разделов и последующим сбоем базы данных, что привело к появлению непоследовательной информации на GitHub веб-сайте.|
|Что было сделано|Запретили push. Запустили восстановление кластеров БД из резервных копий. После восстановления началась репликация новых данных. Увеличили пулл реплик чтения|
|Причина|21 октября в 22:52 UTC в результате регламентных работ по замене вышедшего из строя оптического оборудования 100G была потеряна связь между сетевым концентратором и основным центром обработки данных. |
|Воздействие|Никакие пользовательские данные не были потеряны. В течение большей части инцидента GitHub не мог обслуживать события webhook или создавать и публиковать сайты GitHub Pages. Инцидент повлиял только на метаданные веб-сайтов, хранящиеся в наших базах данных MySQL, такие как Issue и PR. Данные репозитория Git остаются неизменными и доступны на протяжении всего инцидента.|
|Обнаружение|Внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. Инцидент был обнаружен с помощью мониторинга дежурным инженером.|
|Реакция||
|Восстановление||
|Таймлайн| 22:52 - плановые работы по тех обслуживанию <br/> 22:52 - Raft начал процесс отмены выбора руководства. Центр обработки данных и узлы Orchestrator общедоступного облака смогли установить кворум и начать аварийное переключение кластеров для направления операций записи в центр обработки данных. Orchestrator приступил к организации топологии кластера базы данных. Когда подключение было восстановлено, уровень приложений начал направлять трафик записи на новые первичные серверы на сайте West Coast. Серверы баз данных содержали короткий период записи, который не был реплицирован на объект. Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали записи, которых не было в другом центре обработки данных, GitHub не смогли безопасно выполнить возврат основного сервера в центр обработки данных.<br/> 22:54 - Внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. <br/> 23:02 - инженеры группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии. Запрос API Orchestrator показал топологию репликации базы данных, которая включала только серверы из центра обработки данных. <br/> 23:07 - команда решила вручную заблокировать внутренний инструмент развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений. <br/> 23:09 - команда реагирования перевела сайт в желтый статус. Это действие автоматически превратило ситуацию в активный инцидент и отправило предупреждение координатору инцидента. <br/> 23:11 - координатор инцидента присоединился и через две минуты изменил статус решения на красный. <br/> 23:13 Выяснили, что проблема затронула несколько кластеров баз данных. Были вызваны дополнительные инженеры из команды разработчиков баз данных GitHub. <br/> 23:19 - После запроса состояния кластеров базы данных было принято решение прекратить выполнение заданий, которые записывают метаданные о таких вещах, как pushes, приостановив доставку webhook и сборку страниц на GitHub. <br/> 22 октября 00:05 - Инженеры, участвующие в группе реагирования на инциденты, начали разработку плана по устранению несоответствий данных и внедрению наших процедур аварийного переключения для MySQL. Также были проинформированы пользователей о том, что будет выполняться контролируемая отработка отказа внутренней системы хранения данных. <br/>  00:41 - Был инициирован процесс резервного копирования для всех затронутых кластеров MySQL. Одновременно несколько групп инженеров искали способы ускорить передачу и время восстановления без дальнейшего ухудшения удобства использования сайта или риска повреждения данных. <br/>  06:51 - Несколько кластеров завершили восстановление из резервных копий в центре обработки данных и начали репликацию новых данных. Другие более крупные кластеры баз данных все еще восстанавливались. <br/> 07:46 - GitHub опубликовал сообщение в блоге. <br/>  11:12 - Все первичные базы данных снова установлены. GitHub распределили нагрузку чтения по большому пулу реплик чтения, и каждый запрос к GitHub службам имел хорошие шансы попасть в реплику чтения, которая задерживалась на несколько часов. <br/> 13:15 - GitHub приближались к пиковой нагрузке трафика на GitHub.com. GitHub начали предоставлять дополнительные реплики чтения MySQL в общедоступном облаке ранее во время инцидента. Как только они стали доступны, стало проще распределять объем запросов на чтение по большему количеству серверов. Сокращение совокупного использования реплик чтения позволило репликации наверстать упущенное. <br/> 16:24 - Как только реплики были синхронизированы, GitHub выполнили аварийное переключение на исходную топологию, решив немедленные проблемы с задержкой/доступностью, сохранив статус службы красным, пока начали обрабатывать накопившиеся данные. <br/> 16:45 - GitHub повторно включили обработку данных, обработав около 200 000 полезных нагрузок веб-перехватчиков, которые пережили внутренний TTL и были удалены. Обнаружив это, GitHub приостановили эту обработку и внесли изменение, чтобы на время увеличить TTL. <br/> 22 октября 2018 23:03 - Все ожидающие сборки вебхуков и страниц были обработаны, и была подтверждена целостность и правильная работа всех систем. Статус сайта был обновлен до зеленого.|
|Последующие действия|Во время восстановления GitHub просматривал двоичные журналы MySQL, содержащие записи, которые не были реплицированы на сайт из каждого затронутого кластера. Также GitHub проводил анализ журналов и определил, какие записи могут быть автоматически выверены, а какие потребуют разъяснительной работы с пользователями. В этой работе было задействовано несколько команд, и анализ уже определил категорию записей, которые с тех пор повторялись пользователем и успешно сохранялись. <br/> GitHub сделали несколько публичных оценок времени ремонта на основе скорости обработки невыполненных данных. <br/> В ходе анализа был выявлен ряд технических инициатив: <br/> Измените конфигурацию Orchestrator, чтобы предотвратить распространение первичных данных базы данных через региональные границы. Действия Orchestrator выполнялись в соответствии с настройками, несмотря на то, что уровень приложений не смог поддержать это изменение топологии. Выборы лидера в регионе, как правило, безопасны, но внезапное введение задержки в пересечении границы стало основным фактором, способствовавшим этому инциденту. <br/> GitHub ускорили переход на новый механизм отчетности о статусе, который предоставит более богатый форум для обсуждения активных инцидентов более четким и понятным языком. За несколько недель до инцидента GitHub запустили общекорпоративную инженерную инициативу по поддержке обслуживания трафика GitHub из нескольких центров обработки данных в режиме active/неактивный/неактивный. <br/> GitHub будет более тщательнее следить за их приложениями. <br/> Необходимо жесточение операционного контроля или увеличение времени отклика, но это все же не являются достаточными гарантиями надежности сайта. Вдобавок вводится систематическая практика проверки сценариев сбоев до того, как они смогут повлиять на клиента.|








